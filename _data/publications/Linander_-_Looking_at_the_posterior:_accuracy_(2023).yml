DOI: 10.1088/2632-2153/ad0ab4
URL: https://iopscience.iop.org/article/10.1088/2632-2153/ad0ab4
abstract: "Bayesian inference can quantify uncertainty in the\
  \ predictions of neural networks using posterior distributions for model parameters\
  \ and network output. By looking at these posterior distributions, one can separate\
  \ the origin of uncertainty into aleatoric and epistemic contributions. One goal\
  \ of uncertainty quantification is to inform on prediction accuracy. Here we show\
  \ that prediction accuracy depends on both epistemic and aleatoric uncertainty in\
  \ an intricate fashion that cannot be understood in terms of marginalized uncertainty\
  \ distributions alone. How the accuracy relates to epistemic and aleatoric uncertainties\
  \ depends not only on the model architecture, but also on the properties of the\
  \ dataset. We discuss the significance of these results for active learning and\
  \ introduce a novel acquisition function that outperforms common uncertainty-based\
  \ methods. To arrive at our results, we approximated the posteriors using deep ensembles,\
  \ for fully-connected, convolutional and attention-based neural networks."
author:
- family: Linander
  given: Hampus
- family: Balabanov
  given: Oleksandr
- family: Yang
  given: Henry
- family: Mehlig
  given: Bernhard
issued: 2023
publisher: 'Machine Learning: Science and Technology'
title: 'Looking at the posterior: accuracy and uncertainty of neural-network predictions'
type: article-journal
